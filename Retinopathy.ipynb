{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Retinopathy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MittalNeha/retinopathy-kaggle/blob/master/Retinopathy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5hbhyjaow9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34sbLmuW9QM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_kaggle_data():\n",
        "  os.environ['KAGGLE_USERNAME'] = \"mittalneha\"\n",
        "  os.environ['KAGGLE_KEY'] = \"59ec3f992f5fb4b510bebd8dea889381\"\n",
        "  \n",
        "  !kaggle competitions download -c aptos2019-blindness-detection\n",
        "  !kaggle datasets download -d keras/vgg16\n",
        "  \n",
        "  !mkdir kaggle-data\n",
        "  !mv sample_submission.csv kaggle-data/\n",
        "  !mv test.csv kaggle-data/\n",
        "  !mv train.csv kaggle-data/\n",
        "  !unzip test_images.zip -d kaggle-data/test_images\n",
        "  !unzip train_images.zip -d kaggle-data/train_images\n",
        "  \n",
        "  !unzip vgg16.zip -d kaggle-data/vgg16/\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZWWQIcaCUiG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "edc0db3d-9ee9-4e3e-c9b1-50622db56b00"
      },
      "source": [
        "data_dir = \"kaggle-data\"\n",
        "get_kaggle_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train_images.zip to /content\n",
            " 49% 3.95G/8.01G [00:33<00:32, 136MB/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDF4tVHX-hii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import os\n",
        "import glob\n",
        "import h5py\n",
        "import shutil\n",
        "import imgaug as aug\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mimg\n",
        "import imgaug.augmenters as iaa\n",
        "from os import listdir, makedirs, getcwd, remove\n",
        "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, SeparableConv2D\n",
        "from keras.layers import GlobalMaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
        "import cv2\n",
        "from keras import backend as K\n",
        "color = sns.color_palette()\n",
        "%matplotlib inline\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fKC-e_PDaKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(os.listdir(data_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYGTQNJGBHBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Set the seed for hash based operations in python\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "# Set the numpy seed\n",
        "np.random.seed(111)\n",
        "\n",
        "# Disable multi-threading in tensorflow ops\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "\n",
        "# Set the random seed in tensorflow at graph level\n",
        "tf.set_random_seed(111)\n",
        "\n",
        "# Define a tensorflow session with above session configs\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "\n",
        "# Set the session in keras\n",
        "K.set_session(sess)\n",
        "\n",
        "# Make the augmentation sequence deterministic\n",
        "aug.seed(111)\n",
        "\n",
        "seed = 111"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b62u52mWBPbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data_dir = Path(data_dir)\n",
        "\n",
        "# Path to train directory (Fancy pathlib...no more os.path!!)\n",
        "train_dir = data_dir / 'train_images'\n",
        "\n",
        "# Path to validation directory\n",
        "#val_dir = data_dir / 'val'\n",
        "\n",
        "# Path to test directory\n",
        "test_dir = data_dir / 'test_images'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjiqiXqCFilK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv(data_dir/'train.csv')\n",
        "train_data.head()\n",
        "train_data.columns = ['id_code', 'label']\n",
        "train_data.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "360Ww8Hugnjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb3hSZYyxZ8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_image(img):\n",
        "  \n",
        "  if img.shape[2] ==1:\n",
        "    img = np.dstack([img, img, img])\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "  img1 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  ret, img2 = cv2.threshold(img1, 10, 255, cv2.THRESH_BINARY)\n",
        "  points = np.argwhere(img2!=0)\n",
        "  points = np.fliplr(points)\n",
        "  x, y, w, h = cv2.boundingRect(points)\n",
        "\n",
        "#   img_cropped = img1[y:y+h, x:x+w]\n",
        "  color_cropped = img[y:y+h, x:x+w]\n",
        "\n",
        "  clahe = cv2.createCLAHE(clipLimit=4.0)\n",
        "#   img3 = clahe.apply(img_cropped)\n",
        "  lab = cv2.cvtColor(color_cropped, cv2.COLOR_BGR2LAB)\n",
        "  lab_planes = cv2.split(lab)\n",
        "  lab_planes[0] = clahe.apply(lab_planes[0])\n",
        "  lab_planes[1] = clahe.apply(lab_planes[1])\n",
        "  lab_planes[2] = clahe.apply(lab_planes[2])\n",
        "  \n",
        "\n",
        "#   plt.imshow(lab_planes[0], 'gray')\n",
        "\n",
        "  out = cv2.merge(lab_planes)\n",
        "  img3 = cv2.cvtColor(out, cv2.COLOR_LAB2BGR)\n",
        "  \n",
        "  img = cv2.resize(img3, (224,224), cv2.INTER_AREA)\n",
        "  \n",
        "  img = img.astype(np.float32)/255\n",
        "#   img = np.expand_dims(img, axis=2)\n",
        "  \n",
        "  return img\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55l0FIHyFrLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, val = train_test_split(train_data, test_size=0.15)\n",
        "\n",
        "valid_data = []\n",
        "valid_labels = []\n",
        "\n",
        "for idx, row in val.iterrows():\n",
        "    path = str(train_dir) + '/' + row['id_code']+'.png'\n",
        "    img = cv2.imread(path)\n",
        "    img = preprocess_image(img)\n",
        "#     img = img.astype(np.float32)/255.\n",
        "    label = to_categorical(int(row['label']), num_classes=5)\n",
        "    valid_data.append(img)\n",
        "    valid_labels.append(label)\n",
        "    \n",
        "valid_data = np.array(valid_data)\n",
        "valid_labels = np.array(valid_labels)\n",
        "\n",
        "print(\"Total number of validation examples: \", valid_data.shape)\n",
        "print(\"Total number of labels:\", valid_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxVQh245Fs0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = train\n",
        "train_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSiYE5pjF-Gi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Augmentation sequence \n",
        "# seq = iaa.OneOf([\n",
        "#     iaa.Fliplr(), # horizontal flips\n",
        "#     iaa.Affine(rotate=20), # roatation\n",
        "#     iaa.Multiply((1.2, 1.5))]) #random brightness"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTksLOR500va",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq = iaa.OneOf([\n",
        "    iaa.Fliplr(), # horizontal flips\n",
        "    iaa.Affine(rotate=20), # roatation\n",
        "    iaa.Affine(scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)})\n",
        "    ]) #random brightness"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-DGGzcKGB0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_gen(data, batch_size, mode='train'):\n",
        "    # Get total number of samples in the data\n",
        "    n = len(data)\n",
        "    steps = n//batch_size\n",
        "    \n",
        "    # Define two numpy arrays for containing batch data and labels\n",
        "    batch_data = np.zeros((batch_size, 224, 224, 3), dtype=np.float32)\n",
        "    batch_labels = np.zeros((batch_size,5), dtype=np.float32)\n",
        "\n",
        "    # Get a numpy array of all the indices of the input data\n",
        "    indices = np.arange(n)\n",
        "    \n",
        "    # Initialize a counter\n",
        "    i =0\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "        # Get the next batch \n",
        "        count = 0\n",
        "        next_batch = indices[(i*batch_size):(i+1)*batch_size]\n",
        "        for j, idx in enumerate(next_batch):\n",
        "            img_name = str(train_dir) + '/' + data.iloc[idx]['id_code']+'.png'\n",
        "            \n",
        "            label = data.iloc[idx]['label']\n",
        "            \n",
        "            # one hot encoding\n",
        "            encoded_label = to_categorical(label, num_classes=5)\n",
        "            # read the image and resize\n",
        "            img = cv2.imread(str(img_name))\n",
        "#             img = cv2.resize(img, (224,224))\n",
        "            \n",
        "            orig_img = preprocess_image(img)\n",
        "            # normalize the image pixels\n",
        "#             orig_img = orig_img.astype(np.float32)/255.\n",
        "            \n",
        "            batch_data[count] = orig_img\n",
        "            batch_labels[count] = encoded_label\n",
        "            \n",
        "            # generating more samples of the undersampled class\n",
        "#             if label==0 and count < batch_size-2:\n",
        "            if label!=0 and label!=2 and count < batch_size-3:\n",
        "#             if label!=0 and count < batch_size-3 and mode=='train':\n",
        "                aug_img1 = seq.augment_image(img)\n",
        "                aug_img2 = seq.augment_image(img)\n",
        "        \n",
        "                aug_img1 = preprocess_image(aug_img1)\n",
        "                aug_img2 = preprocess_image(aug_img2)\n",
        "            \n",
        "#                 aug_img1 = aug_img1.astype(np.float32)/255.\n",
        "#                 aug_img2 = aug_img2.astype(np.float32)/255.\n",
        "\n",
        "                batch_data[count+1] = aug_img1\n",
        "                batch_labels[count+1] = encoded_label\n",
        "                batch_data[count+2] = aug_img2\n",
        "                batch_labels[count+2] = encoded_label\n",
        "                count +=3            \n",
        "            else:\n",
        "                count+=1\n",
        "\n",
        "            if count>=batch_size-1:\n",
        "                break\n",
        "            \n",
        "        i+=1\n",
        "        \n",
        "        yield batch_data, batch_labels\n",
        "            \n",
        "        if i>=steps:\n",
        "#             print(count)\n",
        "#             for kk in range(0,count):\n",
        "#                 plt.figure()\n",
        "#                 plt.imshow(batch_data[kk])\n",
        "#                 plt.title(str(kk))\n",
        "#             return\n",
        "            i=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XslYRaLeGGxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    input_img = Input(shape=(224,224,3), name='ImageInput')\n",
        "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_1')(input_img)\n",
        "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_2')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool1')(x)\n",
        "    \n",
        "    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_1')(x)\n",
        "    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_2')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool2')(x)\n",
        "    \n",
        "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_1')(x)\n",
        "    x = BatchNormalization(name='bn1')(x)\n",
        "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_2')(x)\n",
        "    x = BatchNormalization(name='bn2')(x)\n",
        "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_3')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool3')(x)\n",
        "    \n",
        "    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_1')(x)\n",
        "    x = BatchNormalization(name='bn3')(x)\n",
        "    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_2')(x)\n",
        "    x = BatchNormalization(name='bn4')(x)\n",
        "    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_3')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool4')(x)\n",
        "    \n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(1024, activation='relu', name='fc1')(x)\n",
        "    x = Dropout(0.7, name='dropout1')(x)\n",
        "    x = Dense(512, activation='relu', name='fc2')(x)\n",
        "    x = Dropout(0.5, name='dropout2')(x)\n",
        "    x = Dense(5, activation='softmax', name='fc3')(x)\n",
        "    \n",
        "    model = Model(inputs=input_img, outputs=x)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH4HU114GIHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model =  build_model()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0o1ZKw3GKaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Open the VGG16 weight file\n",
        "f = h5py.File('kaggle-data/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', 'r')\n",
        "\n",
        "# Select the layers for which you want to set weight.\n",
        "\n",
        "w,b = f['block1_conv1']['block1_conv1_W_1:0'], f['block1_conv1']['block1_conv1_b_1:0']\n",
        "model.layers[1].set_weights = [w,b]\n",
        "\n",
        "w,b = f['block1_conv2']['block1_conv2_W_1:0'], f['block1_conv2']['block1_conv2_b_1:0']\n",
        "model.layers[2].set_weights = [w,b]\n",
        "\n",
        "w,b = f['block2_conv1']['block2_conv1_W_1:0'], f['block2_conv1']['block2_conv1_b_1:0']\n",
        "model.layers[4].set_weights = [w,b]\n",
        "\n",
        "w,b = f['block2_conv2']['block2_conv2_W_1:0'], f['block2_conv2']['block2_conv2_b_1:0']\n",
        "model.layers[5].set_weights = [w,b]\n",
        "\n",
        "f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK_cKafJ53np",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.save_weights('init_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyvAf3pAQpqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir kaggle-data/model/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsKQsKgIetT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = Adam(lr=0.0001, decay=1e-5)\n",
        "es = EarlyStopping(patience=5)\n",
        "# filepath=\"kaggle-data/model/weights-improvement-{epoch:02d}.hdf5\"\n",
        "filepath=\"kaggle-data/model/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "chkpt = ModelCheckpoint(filepath=filepath, save_best_only=True, save_weights_only=True)\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpFZmBTwcHz9",
        "colab_type": "text"
      },
      "source": [
        "**k-fold crossvalidation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILfLXwcC9zZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCAfnSsm937P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# del model\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfCxfQB7oHOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from matplotlib import pyplot\n",
        "# pyplot.plot(history.history['loss'], label='train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwXPuejIcs4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(history.history['loss'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QECy6f1bITmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16\n",
        "nb_epochs = 20\n",
        "\n",
        "# Get a train data generator\n",
        "train_data_gen = data_gen(data=train_data, batch_size=batch_size)\n",
        "\n",
        "# Define the number of training steps\n",
        "nb_train_steps = train_data.shape[0]//batch_size\n",
        "\n",
        "print(\"Number of training and validation steps: {} and {}\".format(nb_train_steps, len(valid_data)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSQ09bWV0n4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_labels.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJYrUvt6eYMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_model(hist):\n",
        "  \n",
        "  #Plot the curves\n",
        "  N = len(hist.history['loss'])\n",
        "  plt.style.use(\"ggplot\")\n",
        "  plt.figure()\n",
        "  plt.plot(np.arange(0, N), hist.history[\"loss\"], label=\"train_loss\")\n",
        "  plt.plot(np.arange(0, N), hist.history[\"val_loss\"], label=\"val_loss\")\n",
        "  plt.plot(np.arange(0, N), hist.history[\"acc\"], label=\"train_acc\")\n",
        "  plt.plot(np.arange(0, N), hist.history[\"val_acc\"], label=\"val_acc\")\n",
        "  plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "  plt.xlabel(\"Epoch #\")\n",
        "  plt.ylabel(\"Loss/Accuracy\")\n",
        "  plt.legend(loc=\"lower left\")\n",
        "  \n",
        "  \n",
        "  #confusion matrix\n",
        "  preds = model.predict(eval_img, batch_size=16)\n",
        "  preds = np.argmax(preds, axis=-1)\n",
        "  \n",
        "  cm  = confusion_matrix(orig_eval_labels, preds)\n",
        "  plt.figure()\n",
        "  plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True, cmap=plt.cm.Blues, show_normed=True, show_absolute=False)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBaR_OzwkGiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM4i6zZ9kI8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# eval_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ICyZ6T4kM9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# val_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am6C4QiQWAik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_data, val_data = train_test_split(train_data, test_size=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwTP7z7VWOX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluate with train data itself\n",
        "eval_img = []\n",
        "eval_labels = []\n",
        "\n",
        "for idx, row in eval_data.iterrows():\n",
        "    path = str(train_dir) + '/' + row['id_code']+'.png'\n",
        "    img = cv2.imread(path)\n",
        "    img = preprocess_image(img)\n",
        "    label = to_categorical(int(row['label']), num_classes=5)\n",
        "    eval_img.append(img)\n",
        "    eval_labels.append(label)\n",
        "    \n",
        "eval_img = np.array(eval_img)\n",
        "eval_labels = np.array(eval_labels)\n",
        "\n",
        "# Original labels\n",
        "orig_eval_labels = np.argmax(eval_labels, axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GURph7mIGhlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.utils import class_weight\n",
        "# class_weight = class_weight.compute_class_weight('balanced', np.unique(orig_train_labels), orig_train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUG2nSKT8Udy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# kappa_metrics = Metrics()\n",
        "history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                               validation_data=(valid_data, valid_labels),callbacks=[es, chkpt])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blEa0u5kbpW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(history.history['loss'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3gIa3p7GNfE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "855357fc-81e5-455f-8c36-01aa8e6557bd"
      },
      "source": [
        "# Fit the model\n",
        "history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                               validation_data=(valid_data, valid_labels),callbacks=[es, chkpt],\n",
        "                              class_weight={0:0.4040466, 1:1.9439528, 2:0.74128234, 3:3.89940828, 4:2.46816479})\n",
        "\n",
        "check_model(history)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "205/205 [==============================] - 918s 4s/step - loss: 1.8907 - acc: 0.2253 - val_loss: 1.8837 - val_acc: 0.2997\n",
            "Epoch 2/20\n",
            "205/205 [==============================] - 906s 4s/step - loss: 1.9178 - acc: 0.1811 - val_loss: 1.9246 - val_acc: 0.0654\n",
            "Epoch 3/20\n",
            "205/205 [==============================] - 909s 4s/step - loss: 1.8913 - acc: 0.1433 - val_loss: 1.9388 - val_acc: 0.0845\n",
            "Epoch 4/20\n",
            "205/205 [==============================] - 926s 5s/step - loss: 1.9083 - acc: 0.1305 - val_loss: 1.9512 - val_acc: 0.0654\n",
            "Epoch 5/20\n",
            "205/205 [==============================] - 933s 5s/step - loss: 1.8823 - acc: 0.1226 - val_loss: 1.9544 - val_acc: 0.0763\n",
            "Epoch 6/20\n",
            "205/205 [==============================] - 933s 5s/step - loss: 1.8293 - acc: 0.1201 - val_loss: 1.9407 - val_acc: 0.0654\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-eadb2a92eab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                               class_weight={0:0.4040466, 1:1.9439528, 2:0.74128234, 3:3.89940828, 4:2.46816479})\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcheck_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-dc59c7af41bb>\u001b[0m in \u001b[0;36mcheck_model\u001b[0;34m(hist)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m#confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'eval_img' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEaCAYAAADpMdsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0dg8iIhINDYPIiISjc2DiIhE+w8MfBYlZ+JSLwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC2zBuEnILuD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "4e89ac94-b838-4ff4-c69e-2a27a63f6b53"
      },
      "source": [
        "history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                               validation_data=(valid_data, valid_labels),callbacks=[es, chkpt], initial_epoch=len(history.history['loss']),\n",
        "                              class_weight={0:0.4040466, 1:1.9439528, 2:0.74128234, 3:3.89940828, 4:2.46816479})\n",
        "\n",
        "check_model(history)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/20\n",
            "205/205 [==============================] - 911s 4s/step - loss: 1.8651 - acc: 0.1405 - val_loss: 1.9351 - val_acc: 0.0654\n",
            "Epoch 8/20\n",
            "205/205 [==============================] - 915s 4s/step - loss: 1.8507 - acc: 0.1189 - val_loss: 1.9276 - val_acc: 0.0654\n",
            "Epoch 9/20\n",
            "205/205 [==============================] - 907s 4s/step - loss: 1.8965 - acc: 0.1043 - val_loss: 1.9348 - val_acc: 0.0654\n",
            "Epoch 10/20\n",
            "176/205 [========================>.....] - ETA: 2:13 - loss: 1.7917 - acc: 0.1286"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-5bf90bbc926f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n\u001b[1;32m      2\u001b[0m                                \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchkpt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                               class_weight={0:0.4040466, 1:1.9439528, 2:0.74128234, 3:3.89940828, 4:2.46816479})\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcheck_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9GFZcY1GwCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check_model(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQJk_BgVzajr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                               validation_data=(valid_data, valid_labels),callbacks=[es, chkpt], initial_epoch=len(history.history['loss'],\n",
        "                              class_weight={0:0.4040466, 1:1.9439528, 2:0.74128234, 3:3.89940828, 4:2.46816479})\n",
        "\n",
        "check_model(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cJYR_bsm-hx",
        "colab_type": "text"
      },
      "source": [
        "**k-fold**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB714TT925YX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold_history = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoLBXbGp-mxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# K.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHDYd4XlcHbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "batch_size = 16\n",
        "nb_epochs = 20\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "# kappa_metrics = Metrics()\n",
        "i = 0\n",
        "init_epoch = 0\n",
        "\n",
        "for train_idx, val_idx in kfold.split(train_data, Y):\n",
        "  print(train_idx)\n",
        "  train_data_gen = data_gen(data=train_data.iloc[train_idx], batch_size=batch_size)\n",
        "  val_data_gen = data_gen(data=train_data.iloc[val_idx], batch_size=batch_size, mode='validation')\n",
        "  \n",
        "  nb_train_steps = len(train_data.iloc[train_idx]) //batch_size\n",
        "  nb_val_steps = len(train_data.iloc[val_idx]) //batch_size\n",
        "  \n",
        "#   model =  build_model()\n",
        "  model.load_weights('init_weights.h5')\n",
        "  model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=opt)\n",
        "  \n",
        "  r = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                               validation_data=val_data_gen,\n",
        "                                validation_steps=nb_val_steps, callbacks=[es, chkpt], \n",
        "                                class_weight={0:0.4040466, 1:1.9439528, 2:0.74128234, 3:3.89940828, 4:2.46816479})\n",
        "  kfold_history.append(r)\n",
        "#   del model\n",
        "  gc.collect()\n",
        "#   init_epoch += len(history.history['loss'])\n",
        "#   print(\"init_epoch = {}\".format(init_epoch))\n",
        "  i+=1\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvFwi4_w8w5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold_history.update(history.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWHAxWRM82vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSfTbRPfoVTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history.history['acc']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJsUqS3LvR2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def histogram(ratings, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Returns the counts of each type of rating that a rater made\n",
        "    \"\"\"\n",
        "    if min_rating is None:\n",
        "        min_rating = min(ratings)\n",
        "    if max_rating is None:\n",
        "        max_rating = max(ratings)\n",
        "    num_ratings = int(max_rating - min_rating + 1)\n",
        "    hist_ratings = [0 for x in range(num_ratings)]\n",
        "    for r in ratings:\n",
        "        hist_ratings[r - min_rating] += 1\n",
        "    return hist_ratings\n",
        "  \n",
        "def quadratic_weighted_kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Calculates the quadratic weighted kappa\n",
        "    quadratic_weighted_kappa calculates the quadratic weighted kappa\n",
        "    value, which is a measure of inter-rater agreement between two raters\n",
        "    that provide discrete numeric ratings.  Potential values range from -1\n",
        "    (representing complete disagreement) to 1 (representing complete\n",
        "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
        "    chance.\n",
        "\n",
        "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
        "    each correspond to a list of integer ratings.  These lists must have the\n",
        "    same length.\n",
        "\n",
        "    The ratings should be integers, and it is assumed that they contain\n",
        "    the complete range of possible ratings.\n",
        "\n",
        "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
        "    is the minimum possible rating, and max_rating is the maximum possible\n",
        "    rating\n",
        "    \"\"\"\n",
        "    rater_a = np.array(rater_a, dtype=int)\n",
        "    rater_b = np.array(rater_b, dtype=int)\n",
        "    assert(len(rater_a) == len(rater_b))\n",
        "    if min_rating is None:\n",
        "        min_rating = min(min(rater_a), min(rater_b))\n",
        "    if max_rating is None:\n",
        "        max_rating = max(max(rater_a), max(rater_b))\n",
        "    print(min_rating.shape)\n",
        "    print(max_rating.shape)\n",
        "    conf_mat = confusion_matrix(rater_a, rater_b)\n",
        "    num_ratings = len(conf_mat)\n",
        "    num_scored_items = float(len(rater_a))\n",
        "\n",
        "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
        "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
        "\n",
        "    numerator = 0.0\n",
        "    denominator = 0.0\n",
        "\n",
        "    for i in range(num_ratings):\n",
        "        for j in range(num_ratings):\n",
        "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
        "                              / num_scored_items)\n",
        "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
        "            numerator += d * conf_mat[i][j] / num_scored_items\n",
        "            denominator += d * expected_count / num_scored_items\n",
        "\n",
        "    return 1.0 - numerator / denominator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umBWXEqpvVAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Metrics(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.val_kappas = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        X_val, y_val = self.validation_data[:2]\n",
        "        y_val = np.argmax(y_val, axis=-1)\n",
        "#         y_val = y_val.astype(int).sum(axis=1) - 1\n",
        "        print(y_val.shape)\n",
        "        \n",
        "        y_pred = self.model.predict(X_val)\n",
        "        y_pred = np.argmax(y_pred, axis=-1)\n",
        "        print(y_pred.shape)\n",
        "#         print(y_pred[0])\n",
        "#         y_pred = y_pred.astype(int).sum(axis=1) - 1\n",
        "        \n",
        "#         print(y_pred.shape)\n",
        "        _val_kappa = quadratic_weighted_kappa(y_val, y_pred)\n",
        "\n",
        "#         _val_kappa = cohen_kappa_score(\n",
        "#             y_val,\n",
        "#             y_pred, \n",
        "#             weights='quadratic'\n",
        "#         )\n",
        "\n",
        "        self.val_kappas.append(_val_kappa)\n",
        "\n",
        "        print(f\"val_kappa: {_val_kappa:.4f}\")\n",
        "        \n",
        "        if _val_kappa == max(self.val_kappas):\n",
        "            print(\"Validation Kappa has improved. Saving model.\")\n",
        "            self.model.save('kaggle-data/model/model.h5')\n",
        "\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}