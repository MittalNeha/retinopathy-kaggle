{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Retinopathy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MittalNeha/retinopathy-kaggle/blob/master/Retinopathy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5hbhyjaow9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34sbLmuW9QM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_kaggle_data():\n",
        "  os.environ['KAGGLE_USERNAME'] = \"mittalneha\"\n",
        "  os.environ['KAGGLE_KEY'] = \"59ec3f992f5fb4b510bebd8dea889381\"\n",
        "  \n",
        "  !kaggle competitions download -c aptos2019-blindness-detection\n",
        "  !kaggle datasets download -d keras/vgg16\n",
        "  \n",
        "  !mkdir kaggle-data\n",
        "  !mv sample_submission.csv kaggle-data/\n",
        "  !mv test.csv kaggle-data/\n",
        "  !mv train.csv kaggle-data/\n",
        "  !unzip test_images.zip -d kaggle-data/test_images\n",
        "  !unzip train_images.zip -d kaggle-data/train_images\n",
        "  \n",
        "  !unzip vgg16.zip -d kaggle-data/vgg16/\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZWWQIcaCUiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = \"kaggle-data\"\n",
        "# get_kaggle_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDF4tVHX-hii",
        "colab_type": "code",
        "outputId": "31f9f0ef-52ae-4fd7-cb00-68cbbc21191b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import os\n",
        "import glob\n",
        "import h5py\n",
        "import shutil\n",
        "import imgaug as aug\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mimg\n",
        "import imgaug.augmenters as iaa\n",
        "from os import listdir, makedirs, getcwd, remove\n",
        "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, SeparableConv2D\n",
        "from keras.layers import GlobalMaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
        "import cv2\n",
        "from keras import backend as K\n",
        "color = sns.color_palette()\n",
        "%matplotlib inline\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fKC-e_PDaKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(os.listdir(data_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYGTQNJGBHBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Set the seed for hash based operations in python\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "# Set the numpy seed\n",
        "np.random.seed(111)\n",
        "\n",
        "# Disable multi-threading in tensorflow ops\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "\n",
        "# Set the random seed in tensorflow at graph level\n",
        "tf.set_random_seed(111)\n",
        "\n",
        "# Define a tensorflow session with above session configs\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "\n",
        "# Set the session in keras\n",
        "K.set_session(sess)\n",
        "\n",
        "# Make the augmentation sequence deterministic\n",
        "aug.seed(111)\n",
        "\n",
        "seed = 111"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b62u52mWBPbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data_dir = Path(data_dir)\n",
        "\n",
        "# Path to train directory (Fancy pathlib...no more os.path!!)\n",
        "train_dir = data_dir / 'train_images'\n",
        "\n",
        "# Path to validation directory\n",
        "#val_dir = data_dir / 'val'\n",
        "\n",
        "# Path to test directory\n",
        "test_dir = data_dir / 'test_images'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjiqiXqCFilK",
        "colab_type": "code",
        "outputId": "558a435b-a76c-413e-a0ce-08bebd2253bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data = pd.read_csv(data_dir/'train.csv')\n",
        "train_data.head()\n",
        "train_data.columns = ['id_code', 'label']\n",
        "train_data.columns"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id_code', 'label'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "360Ww8Hugnjp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "caacc517-c8b5-4731-d96d-1c14ba38bd7a"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3662, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b_lNAEmgq84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = train_data['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uD5CP91gvLj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a471ad92-e576-4c3a-ccc3-3c1e5ea6c17d"
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3662,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55l0FIHyFrLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train, val = train_test_split(train_data, test_size=0.1)\n",
        "\n",
        "# valid_data = []\n",
        "# valid_labels = []\n",
        "\n",
        "# for idx, row in val.iterrows():\n",
        "#     path = str(train_dir) + '/' + row['id_code']+'.png'\n",
        "#     img = cv2.imread(path)\n",
        "#     img = cv2.resize(img, (224,224))\n",
        "#     if img.shape[2] ==1:\n",
        "#         img = np.dstack([img, img, img])\n",
        "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "#     img = img.astype(np.float32)/255.\n",
        "#     label = to_categorical(int(row['label']), num_classes=5)\n",
        "#     valid_data.append(img)\n",
        "#     valid_labels.append(label)\n",
        "    \n",
        "# valid_data = np.array(valid_data)\n",
        "# valid_labels = np.array(valid_labels)\n",
        "\n",
        "# print(\"Total number of validation examples: \", valid_data.shape)\n",
        "# print(\"Total number of labels:\", valid_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxVQh245Fs0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_data = train\n",
        "# train_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSiYE5pjF-Gi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Augmentation sequence \n",
        "seq = iaa.OneOf([\n",
        "    iaa.Fliplr(), # horizontal flips\n",
        "    iaa.Affine(rotate=20), # roatation\n",
        "    iaa.Multiply((1.2, 1.5))]) #random brightness"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-DGGzcKGB0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_gen(data, batch_size, mode='train'):\n",
        "    # Get total number of samples in the data\n",
        "    n = len(data)\n",
        "    steps = n//batch_size\n",
        "    \n",
        "    # Define two numpy arrays for containing batch data and labels\n",
        "    batch_data = np.zeros((batch_size, 224, 224, 3), dtype=np.float32)\n",
        "    batch_labels = np.zeros((batch_size,5), dtype=np.float32)\n",
        "\n",
        "    # Get a numpy array of all the indices of the input data\n",
        "    indices = np.arange(n)\n",
        "    \n",
        "    # Initialize a counter\n",
        "    i =0\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "        # Get the next batch \n",
        "        count = 0\n",
        "        next_batch = indices[(i*batch_size):(i+1)*batch_size]\n",
        "        for j, idx in enumerate(next_batch):\n",
        "            img_name = str(train_dir) + '/' + data.iloc[idx]['id_code']+'.png'\n",
        "            \n",
        "            label = data.iloc[idx]['label']\n",
        "            \n",
        "            # one hot encoding\n",
        "            encoded_label = to_categorical(label, num_classes=5)\n",
        "            # read the image and resize\n",
        "            img = cv2.imread(str(img_name))\n",
        "            img = cv2.resize(img, (224,224))\n",
        "            \n",
        "            # check if it's grayscale\n",
        "            if img.shape[2]==1:\n",
        "                img = np.dstack([img, img, img])\n",
        "            \n",
        "            # cv2 reads in BGR mode by default\n",
        "            orig_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            # normalize the image pixels\n",
        "            orig_img = orig_img.astype(np.float32)/255.\n",
        "            \n",
        "            batch_data[count] = orig_img\n",
        "            batch_labels[count] = encoded_label\n",
        "            \n",
        "            # generating more samples of the undersampled class\n",
        "#             if label==0 and count < batch_size-2:\n",
        "#             if label!=0 and label!=2 and count < batch_size-3:\n",
        "            if label!=0 and count < batch_size-3 and mode=='train':\n",
        "                aug_img1 = seq.augment_image(img)\n",
        "                aug_img2 = seq.augment_image(img)\n",
        "                aug_img1 = cv2.cvtColor(aug_img1, cv2.COLOR_BGR2RGB)\n",
        "                aug_img2 = cv2.cvtColor(aug_img2, cv2.COLOR_BGR2RGB)\n",
        "                aug_img1 = aug_img1.astype(np.float32)/255.\n",
        "                aug_img2 = aug_img2.astype(np.float32)/255.\n",
        "\n",
        "                batch_data[count+1] = aug_img1\n",
        "                batch_labels[count+1] = encoded_label\n",
        "                batch_data[count+2] = aug_img2\n",
        "                batch_labels[count+2] = encoded_label\n",
        "                count +=3            \n",
        "            else:\n",
        "                count+=1\n",
        "\n",
        "            if count>=batch_size-1:\n",
        "                break\n",
        "            \n",
        "        i+=1\n",
        "        \n",
        "        yield batch_data, batch_labels\n",
        "            \n",
        "        if i>=steps:\n",
        "#             print(count)\n",
        "#             for kk in range(0,count):\n",
        "#                 plt.figure()\n",
        "#                 plt.imshow(batch_data[kk])\n",
        "#                 plt.title(str(kk))\n",
        "#             return\n",
        "            i=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XslYRaLeGGxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    input_img = Input(shape=(224,224,3), name='ImageInput')\n",
        "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_1')(input_img)\n",
        "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_2')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool1')(x)\n",
        "    \n",
        "    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_1')(x)\n",
        "    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_2')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool2')(x)\n",
        "    \n",
        "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_1')(x)\n",
        "    x = BatchNormalization(name='bn1')(x)\n",
        "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_2')(x)\n",
        "    x = BatchNormalization(name='bn2')(x)\n",
        "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_3')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool3')(x)\n",
        "    \n",
        "    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_1')(x)\n",
        "    x = BatchNormalization(name='bn3')(x)\n",
        "    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_2')(x)\n",
        "    x = BatchNormalization(name='bn4')(x)\n",
        "    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_3')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool4')(x)\n",
        "    \n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(1024, activation='relu', name='fc1')(x)\n",
        "    x = Dropout(0.7, name='dropout1')(x)\n",
        "    x = Dense(512, activation='relu', name='fc2')(x)\n",
        "    x = Dropout(0.5, name='dropout2')(x)\n",
        "    x = Dense(5, activation='softmax', name='fc3')(x)\n",
        "    \n",
        "    model = Model(inputs=input_img, outputs=x)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH4HU114GIHq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bcf1fe5d-6039-468a-fc5b-fc0cb0f899cc"
      },
      "source": [
        "model =  build_model()\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0717 07:02:46.562520 140435849488256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0717 07:02:46.568181 140435849488256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0717 07:02:46.595273 140435849488256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0717 07:02:46.667579 140435849488256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0717 07:02:46.668486 140435849488256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0717 07:02:46.669370 140435849488256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "W0717 07:02:47.267611 140435849488256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0717 07:02:47.771918 140435849488256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0717 07:02:47.772990 140435849488256 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "ImageInput (InputLayer)      (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "Conv1_1 (Conv2D)             (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "Conv1_2 (Conv2D)             (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "Conv2_1 (SeparableConv2D)    (None, 112, 112, 128)     8896      \n",
            "_________________________________________________________________\n",
            "Conv2_2 (SeparableConv2D)    (None, 112, 112, 128)     17664     \n",
            "_________________________________________________________________\n",
            "pool2 (MaxPooling2D)         (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "Conv3_1 (SeparableConv2D)    (None, 56, 56, 256)       34176     \n",
            "_________________________________________________________________\n",
            "bn1 (BatchNormalization)     (None, 56, 56, 256)       1024      \n",
            "_________________________________________________________________\n",
            "Conv3_2 (SeparableConv2D)    (None, 56, 56, 256)       68096     \n",
            "_________________________________________________________________\n",
            "bn2 (BatchNormalization)     (None, 56, 56, 256)       1024      \n",
            "_________________________________________________________________\n",
            "Conv3_3 (SeparableConv2D)    (None, 56, 56, 256)       68096     \n",
            "_________________________________________________________________\n",
            "pool3 (MaxPooling2D)         (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "Conv4_1 (SeparableConv2D)    (None, 28, 28, 512)       133888    \n",
            "_________________________________________________________________\n",
            "bn3 (BatchNormalization)     (None, 28, 28, 512)       2048      \n",
            "_________________________________________________________________\n",
            "Conv4_2 (SeparableConv2D)    (None, 28, 28, 512)       267264    \n",
            "_________________________________________________________________\n",
            "bn4 (BatchNormalization)     (None, 28, 28, 512)       2048      \n",
            "_________________________________________________________________\n",
            "Conv4_3 (SeparableConv2D)    (None, 28, 28, 512)       267264    \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 100352)            0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 1024)              102761472 \n",
            "_________________________________________________________________\n",
            "dropout1 (Dropout)           (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout2 (Dropout)           (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "fc3 (Dense)                  (None, 5)                 2565      \n",
            "=================================================================\n",
            "Total params: 104,199,045\n",
            "Trainable params: 104,195,973\n",
            "Non-trainable params: 3,072\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0o1ZKw3GKaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9524d1f-f027-4b44-ccb7-1c5e1e8050c6"
      },
      "source": [
        "# Open the VGG16 weight file\n",
        "f = h5py.File('kaggle-data/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', 'r')\n",
        "\n",
        "# Select the layers for which you want to set weight.\n",
        "\n",
        "w,b = f['block1_conv1']['block1_conv1_W_1:0'], f['block1_conv1']['block1_conv1_b_1:0']\n",
        "model.layers[1].set_weights = [w,b]\n",
        "\n",
        "w,b = f['block1_conv2']['block1_conv2_W_1:0'], f['block1_conv2']['block1_conv2_b_1:0']\n",
        "model.layers[2].set_weights = [w,b]\n",
        "\n",
        "w,b = f['block2_conv1']['block2_conv1_W_1:0'], f['block2_conv1']['block2_conv1_b_1:0']\n",
        "model.layers[4].set_weights = [w,b]\n",
        "\n",
        "w,b = f['block2_conv2']['block2_conv2_W_1:0'], f['block2_conv2']['block2_conv2_b_1:0']\n",
        "model.layers[5].set_weights = [w,b]\n",
        "\n",
        "f.close()\n",
        "model.summary()    "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "ImageInput (InputLayer)      (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "Conv1_1 (Conv2D)             (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "Conv1_2 (Conv2D)             (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "Conv2_1 (SeparableConv2D)    (None, 112, 112, 128)     8896      \n",
            "_________________________________________________________________\n",
            "Conv2_2 (SeparableConv2D)    (None, 112, 112, 128)     17664     \n",
            "_________________________________________________________________\n",
            "pool2 (MaxPooling2D)         (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "Conv3_1 (SeparableConv2D)    (None, 56, 56, 256)       34176     \n",
            "_________________________________________________________________\n",
            "bn1 (BatchNormalization)     (None, 56, 56, 256)       1024      \n",
            "_________________________________________________________________\n",
            "Conv3_2 (SeparableConv2D)    (None, 56, 56, 256)       68096     \n",
            "_________________________________________________________________\n",
            "bn2 (BatchNormalization)     (None, 56, 56, 256)       1024      \n",
            "_________________________________________________________________\n",
            "Conv3_3 (SeparableConv2D)    (None, 56, 56, 256)       68096     \n",
            "_________________________________________________________________\n",
            "pool3 (MaxPooling2D)         (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "Conv4_1 (SeparableConv2D)    (None, 28, 28, 512)       133888    \n",
            "_________________________________________________________________\n",
            "bn3 (BatchNormalization)     (None, 28, 28, 512)       2048      \n",
            "_________________________________________________________________\n",
            "Conv4_2 (SeparableConv2D)    (None, 28, 28, 512)       267264    \n",
            "_________________________________________________________________\n",
            "bn4 (BatchNormalization)     (None, 28, 28, 512)       2048      \n",
            "_________________________________________________________________\n",
            "Conv4_3 (SeparableConv2D)    (None, 28, 28, 512)       267264    \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 100352)            0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 1024)              102761472 \n",
            "_________________________________________________________________\n",
            "dropout1 (Dropout)           (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout2 (Dropout)           (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "fc3 (Dense)                  (None, 5)                 2565      \n",
            "=================================================================\n",
            "Total params: 104,199,045\n",
            "Trainable params: 104,195,973\n",
            "Non-trainable params: 3,072\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyvAf3pAQpqi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51c7a605-4fe8-44fe-b119-88df31a57b19"
      },
      "source": [
        "!mkdir kaggle-data/model/"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘kaggle-data/model/’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsKQsKgIetT",
        "colab_type": "code",
        "outputId": "1cca66d1-dc90-418f-f636-e8b1ccb4438a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "opt = Adam(lr=0.0001, decay=1e-5)\n",
        "# es = EarlyStopping(patience=5)\n",
        "# filepath=\"kaggle-data/model/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "# chkpt = ModelCheckpoint(filepath=filepath, save_best_only=True, save_weights_only=True)\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=opt)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0717 07:03:03.078920 140435849488256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJsUqS3LvR2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def histogram(ratings, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Returns the counts of each type of rating that a rater made\n",
        "    \"\"\"\n",
        "    if min_rating is None:\n",
        "        min_rating = min(ratings)\n",
        "    if max_rating is None:\n",
        "        max_rating = max(ratings)\n",
        "    num_ratings = int(max_rating - min_rating + 1)\n",
        "    hist_ratings = [0 for x in range(num_ratings)]\n",
        "    for r in ratings:\n",
        "        hist_ratings[r - min_rating] += 1\n",
        "    return hist_ratings\n",
        "  \n",
        "def quadratic_weighted_kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Calculates the quadratic weighted kappa\n",
        "    quadratic_weighted_kappa calculates the quadratic weighted kappa\n",
        "    value, which is a measure of inter-rater agreement between two raters\n",
        "    that provide discrete numeric ratings.  Potential values range from -1\n",
        "    (representing complete disagreement) to 1 (representing complete\n",
        "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
        "    chance.\n",
        "\n",
        "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
        "    each correspond to a list of integer ratings.  These lists must have the\n",
        "    same length.\n",
        "\n",
        "    The ratings should be integers, and it is assumed that they contain\n",
        "    the complete range of possible ratings.\n",
        "\n",
        "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
        "    is the minimum possible rating, and max_rating is the maximum possible\n",
        "    rating\n",
        "    \"\"\"\n",
        "    rater_a = np.array(rater_a, dtype=int)\n",
        "    rater_b = np.array(rater_b, dtype=int)\n",
        "    assert(len(rater_a) == len(rater_b))\n",
        "    if min_rating is None:\n",
        "        min_rating = min(min(rater_a), min(rater_b))\n",
        "    if max_rating is None:\n",
        "        max_rating = max(max(rater_a), max(rater_b))\n",
        "    print(min_rating.shape)\n",
        "    print(max_rating.shape)\n",
        "    conf_mat = confusion_matrix(rater_a, rater_b)\n",
        "    num_ratings = len(conf_mat)\n",
        "    num_scored_items = float(len(rater_a))\n",
        "\n",
        "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
        "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
        "\n",
        "    numerator = 0.0\n",
        "    denominator = 0.0\n",
        "\n",
        "    for i in range(num_ratings):\n",
        "        for j in range(num_ratings):\n",
        "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
        "                              / num_scored_items)\n",
        "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
        "            numerator += d * conf_mat[i][j] / num_scored_items\n",
        "            denominator += d * expected_count / num_scored_items\n",
        "\n",
        "    return 1.0 - numerator / denominator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umBWXEqpvVAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Metrics(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.val_kappas = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        X_val, y_val = self.validation_data[:2]\n",
        "        y_val = np.argmax(y_val, axis=-1)\n",
        "#         y_val = y_val.astype(int).sum(axis=1) - 1\n",
        "        print(y_val.shape)\n",
        "        \n",
        "        y_pred = self.model.predict(X_val)\n",
        "        y_pred = np.argmax(y_pred, axis=-1)\n",
        "        print(y_pred.shape)\n",
        "#         print(y_pred[0])\n",
        "#         y_pred = y_pred.astype(int).sum(axis=1) - 1\n",
        "        \n",
        "#         print(y_pred.shape)\n",
        "        _val_kappa = quadratic_weighted_kappa(y_val, y_pred)\n",
        "\n",
        "#         _val_kappa = cohen_kappa_score(\n",
        "#             y_val,\n",
        "#             y_pred, \n",
        "#             weights='quadratic'\n",
        "#         )\n",
        "\n",
        "        self.val_kappas.append(_val_kappa)\n",
        "\n",
        "        print(f\"val_kappa: {_val_kappa:.4f}\")\n",
        "        \n",
        "        if _val_kappa == max(self.val_kappas):\n",
        "            print(\"Validation Kappa has improved. Saving model.\")\n",
        "            self.model.save('kaggle-data/model/model.h5')\n",
        "\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpFZmBTwcHz9",
        "colab_type": "text"
      },
      "source": [
        "**k-fold crossvalidation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHDYd4XlcHbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "b78e1252-2439-420c-fe5c-136ae9831f40"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "batch_size = 16\n",
        "nb_epochs = 10\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "kappa_metrics = Metrics()\n",
        "i = 0\n",
        "train_data_gen = data_gen(data=train_data, batch_size=batch_size)\n",
        "history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=train_data.shape[0]//nb_epochs, callbacks=[kappa_metrics], initial_epoch=0)\n",
        "i=1\n",
        "\n",
        "for train_idx, val_idx in kfold.split(train_data, Y):\n",
        "#   Y[train_idx]\n",
        "#   train = train_data[train_idx, :]\n",
        "  \n",
        "#   print(train.shape)\n",
        "  print(train_idx)\n",
        "  train_data_gen = data_gen(data=train_data.iloc[train_idx], batch_size=batch_size)\n",
        "  val_data_gen = data_gen(data=train_data.iloc[val_idx], batch_size=batch_size, mode='validation')\n",
        "  \n",
        "  nb_train_steps = len(train_data.iloc[train_idx])\n",
        "  nb_val_steps = len(train_data.iloc[val_idx])\n",
        "  \n",
        "  history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                               validation_data=val_data_gen,\n",
        "                                validation_steps=nb_val_steps, callbacks=[kappa_metrics], initial_epoch=i*nb_epochs)\n",
        "  i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0717 07:03:03.277261 140435849488256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "353/366 [===========================>..] - ETA: 8s - loss: 1.2878 - acc: 0.4671"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QECy6f1bITmV",
        "colab_type": "code",
        "outputId": "672a69df-76bc-4ca0-bc25-f2d5d247d7e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# batch_size = 16\n",
        "# nb_epochs = 20\n",
        "\n",
        "# # Get a train data generator\n",
        "# train_data_gen = data_gen(data=train_data, batch_size=batch_size)\n",
        "\n",
        "# # Define the number of training steps\n",
        "# nb_train_steps = train_data.shape[0]//batch_size\n",
        "\n",
        "# print(\"Number of training and validation steps: {} and {}\".format(nb_train_steps, len(valid_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training and validation steps: 205 and 367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSQ09bWV0n4s",
        "colab_type": "code",
        "outputId": "112d9ac5-91ce-4a77-85d1-90eff60a7b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "valid_labels.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(367, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJYrUvt6eYMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLXUR7AWm6ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GURph7mIGhlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.utils import class_weight\n",
        "# class_weight = class_weight.compute_class_weight('balanced', np.unique(orig_train_labels), orig_train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUG2nSKT8Udy",
        "colab_type": "code",
        "outputId": "bf8d7112-603f-4d8c-c563-00f298d7ca04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "kappa_metrics = Metrics()\n",
        "history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                               validation_data=(valid_data, valid_labels),callbacks=[kappa_metrics])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "205/205 [==============================] - 170s 828ms/step - loss: 0.5974 - acc: 0.7695 - val_loss: 0.6152 - val_acc: 0.7902\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8048\n",
            "Validation Kappa has improved. Saving model.\n",
            "Epoch 2/20\n",
            "205/205 [==============================] - 170s 828ms/step - loss: 0.5488 - acc: 0.7771 - val_loss: 0.6518 - val_acc: 0.7684\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8155\n",
            "Validation Kappa has improved. Saving model.\n",
            "Epoch 3/20\n",
            "205/205 [==============================] - 165s 804ms/step - loss: 0.5657 - acc: 0.7826 - val_loss: 0.6326 - val_acc: 0.7766\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8096\n",
            "Epoch 4/20\n",
            "205/205 [==============================] - 162s 790ms/step - loss: 0.5734 - acc: 0.7762 - val_loss: 0.6482 - val_acc: 0.7902\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8167\n",
            "Validation Kappa has improved. Saving model.\n",
            "Epoch 5/20\n",
            "205/205 [==============================] - 165s 805ms/step - loss: 0.5660 - acc: 0.7753 - val_loss: 0.6478 - val_acc: 0.7629\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.7946\n",
            "Epoch 6/20\n",
            "205/205 [==============================] - 200s 974ms/step - loss: 0.5412 - acc: 0.7930 - val_loss: 0.6544 - val_acc: 0.7711\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8205\n",
            "Validation Kappa has improved. Saving model.\n",
            "Epoch 7/20\n",
            "205/205 [==============================] - 167s 816ms/step - loss: 0.4912 - acc: 0.8049 - val_loss: 0.6815 - val_acc: 0.7738\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8259\n",
            "Validation Kappa has improved. Saving model.\n",
            "Epoch 8/20\n",
            "205/205 [==============================] - 173s 845ms/step - loss: 0.4986 - acc: 0.8055 - val_loss: 0.6455 - val_acc: 0.7956\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8454\n",
            "Validation Kappa has improved. Saving model.\n",
            "Epoch 9/20\n",
            "205/205 [==============================] - 163s 793ms/step - loss: 0.4571 - acc: 0.8213 - val_loss: 0.6939 - val_acc: 0.7711\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8475\n",
            "Validation Kappa has improved. Saving model.\n",
            "Epoch 10/20\n",
            "205/205 [==============================] - 161s 784ms/step - loss: 0.4714 - acc: 0.8232 - val_loss: 0.7429 - val_acc: 0.8038\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8282\n",
            "Epoch 11/20\n",
            "205/205 [==============================] - 162s 790ms/step - loss: 0.4495 - acc: 0.8256 - val_loss: 0.7209 - val_acc: 0.8038\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8417\n",
            "Epoch 12/20\n",
            "205/205 [==============================] - 164s 799ms/step - loss: 0.4238 - acc: 0.8360 - val_loss: 0.7954 - val_acc: 0.7657\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.7898\n",
            "Epoch 13/20\n",
            "205/205 [==============================] - 163s 797ms/step - loss: 0.3929 - acc: 0.8439 - val_loss: 0.7869 - val_acc: 0.7902\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8277\n",
            "Epoch 14/20\n",
            "205/205 [==============================] - 162s 792ms/step - loss: 0.3699 - acc: 0.8552 - val_loss: 0.7196 - val_acc: 0.8011\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8485\n",
            "Validation Kappa has improved. Saving model.\n",
            "Epoch 15/20\n",
            "205/205 [==============================] - 161s 784ms/step - loss: 0.3843 - acc: 0.8607 - val_loss: 0.8424 - val_acc: 0.8065\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8298\n",
            "Epoch 16/20\n",
            "205/205 [==============================] - 162s 789ms/step - loss: 0.3900 - acc: 0.8555 - val_loss: 0.7126 - val_acc: 0.8038\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8469\n",
            "Epoch 17/20\n",
            "205/205 [==============================] - 164s 800ms/step - loss: 0.3708 - acc: 0.8576 - val_loss: 0.7034 - val_acc: 0.8011\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8180\n",
            "Epoch 18/20\n",
            "205/205 [==============================] - 163s 797ms/step - loss: 0.3120 - acc: 0.8826 - val_loss: 0.7800 - val_acc: 0.7738\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8262\n",
            "Epoch 19/20\n",
            "205/205 [==============================] - 165s 804ms/step - loss: 0.3538 - acc: 0.8686 - val_loss: 0.8219 - val_acc: 0.7793\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.8285\n",
            "Epoch 20/20\n",
            "205/205 [==============================] - 162s 792ms/step - loss: 0.3137 - acc: 0.8820 - val_loss: 0.8803 - val_acc: 0.7847\n",
            "(367,)\n",
            "(367,)\n",
            "()\n",
            "()\n",
            "val_kappa: 0.7981\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3gIa3p7GNfE",
        "colab_type": "code",
        "outputId": "8f537bd7-1d55-436a-ed97-80f356b55a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        }
      },
      "source": [
        "# # Fit the model\n",
        "# history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "#                                validation_data=(valid_data, valid_labels),callbacks=[es, chkpt])\n",
        "#                               class_weight={0:0.4040466, 1:1.9439528, 2:0.74128234, 3:3.89940828, 4:2.46816479})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0715 06:53:57.427965 140484095510400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "205/205 [==============================] - 174s 847ms/step - loss: 1.3836 - acc: 0.3756 - val_loss: 1.3904 - val_acc: 0.2997\n",
            "Epoch 2/20\n",
            "205/205 [==============================] - 170s 831ms/step - loss: 1.3707 - acc: 0.3665 - val_loss: 1.3979 - val_acc: 0.2997\n",
            "Epoch 3/20\n",
            "205/205 [==============================] - 168s 819ms/step - loss: 1.3737 - acc: 0.3628 - val_loss: 1.4077 - val_acc: 0.2997\n",
            "Epoch 4/20\n",
            "205/205 [==============================] - 170s 829ms/step - loss: 1.3676 - acc: 0.3869 - val_loss: 1.2339 - val_acc: 0.4414\n",
            "Epoch 5/20\n",
            "205/205 [==============================] - 169s 825ms/step - loss: 1.1964 - acc: 0.5534 - val_loss: 0.9823 - val_acc: 0.6757\n",
            "Epoch 6/20\n",
            "205/205 [==============================] - 164s 799ms/step - loss: 1.0353 - acc: 0.6454 - val_loss: 0.8277 - val_acc: 0.7112\n",
            "Epoch 7/20\n",
            "205/205 [==============================] - 170s 830ms/step - loss: 1.0317 - acc: 0.6247 - val_loss: 0.7871 - val_acc: 0.7139\n",
            "Epoch 8/20\n",
            "205/205 [==============================] - 169s 826ms/step - loss: 1.0201 - acc: 0.6232 - val_loss: 0.7796 - val_acc: 0.7139\n",
            "Epoch 9/20\n",
            "205/205 [==============================] - 166s 810ms/step - loss: 0.9834 - acc: 0.6320 - val_loss: 0.7898 - val_acc: 0.7248\n",
            "Epoch 10/20\n",
            "205/205 [==============================] - 163s 797ms/step - loss: 0.9908 - acc: 0.6277 - val_loss: 0.7563 - val_acc: 0.7139\n",
            "Epoch 11/20\n",
            "205/205 [==============================] - 164s 802ms/step - loss: 0.9606 - acc: 0.6451 - val_loss: 0.7739 - val_acc: 0.7139\n",
            "Epoch 12/20\n",
            "205/205 [==============================] - 163s 795ms/step - loss: 0.9677 - acc: 0.6503 - val_loss: 0.7335 - val_acc: 0.7384\n",
            "Epoch 13/20\n",
            "205/205 [==============================] - 159s 777ms/step - loss: 1.0238 - acc: 0.6082 - val_loss: 0.7908 - val_acc: 0.7057\n",
            "Epoch 14/20\n",
            "205/205 [==============================] - 164s 799ms/step - loss: 0.9766 - acc: 0.6308 - val_loss: 0.8154 - val_acc: 0.7084\n",
            "Epoch 15/20\n",
            "205/205 [==============================] - 162s 792ms/step - loss: 0.9406 - acc: 0.6433 - val_loss: 0.7718 - val_acc: 0.7057\n",
            "Epoch 16/20\n",
            "205/205 [==============================] - 164s 799ms/step - loss: 0.9409 - acc: 0.6479 - val_loss: 0.7420 - val_acc: 0.7112\n",
            "Epoch 17/20\n",
            "205/205 [==============================] - 163s 793ms/step - loss: 0.9355 - acc: 0.6448 - val_loss: 0.7245 - val_acc: 0.7384\n",
            "Epoch 18/20\n",
            "205/205 [==============================] - 158s 773ms/step - loss: 0.9485 - acc: 0.6250 - val_loss: 0.7758 - val_acc: 0.7166\n",
            "Epoch 19/20\n",
            "205/205 [==============================] - 160s 782ms/step - loss: 0.9184 - acc: 0.6430 - val_loss: 0.7497 - val_acc: 0.7139\n",
            "Epoch 20/20\n",
            "205/205 [==============================] - 159s 777ms/step - loss: 0.9144 - acc: 0.6491 - val_loss: 0.7416 - val_acc: 0.7248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC2zBuEnILuD",
        "colab_type": "code",
        "outputId": "b3477f7d-f140-4f2b-c329-e5e8db6b4d25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        }
      },
      "source": [
        "history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                               validation_data=(valid_data, valid_labels),callbacks=[es, chkpt],\n",
        "                              class_weight={0:0.4040466, 1:1.9439528, 2:0.74128234, 3:3.89940828, 4:2.46816479})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "205/205 [==============================] - 162s 791ms/step - loss: 1.5097 - acc: 0.6101 - val_loss: 0.8776 - val_acc: 0.6866\n",
            "Epoch 2/20\n",
            "205/205 [==============================] - 160s 783ms/step - loss: 1.4380 - acc: 0.5713 - val_loss: 0.8613 - val_acc: 0.5695\n",
            "Epoch 3/20\n",
            "205/205 [==============================] - 162s 791ms/step - loss: 1.4194 - acc: 0.5503 - val_loss: 0.8804 - val_acc: 0.6649\n",
            "Epoch 4/20\n",
            "205/205 [==============================] - 164s 801ms/step - loss: 1.4147 - acc: 0.5784 - val_loss: 0.8434 - val_acc: 0.6022\n",
            "Epoch 5/20\n",
            "205/205 [==============================] - 160s 780ms/step - loss: 1.4186 - acc: 0.5716 - val_loss: 0.9395 - val_acc: 0.6676\n",
            "Epoch 6/20\n",
            "205/205 [==============================] - 162s 788ms/step - loss: 1.3841 - acc: 0.6000 - val_loss: 0.8541 - val_acc: 0.6948\n",
            "Epoch 7/20\n",
            "205/205 [==============================] - 158s 771ms/step - loss: 1.2296 - acc: 0.6430 - val_loss: 0.8029 - val_acc: 0.6894\n",
            "Epoch 8/20\n",
            "205/205 [==============================] - 161s 786ms/step - loss: 1.3251 - acc: 0.5970 - val_loss: 0.9060 - val_acc: 0.6676\n",
            "Epoch 9/20\n",
            "205/205 [==============================] - 160s 782ms/step - loss: 1.1907 - acc: 0.6314 - val_loss: 0.7918 - val_acc: 0.6812\n",
            "Epoch 10/20\n",
            "205/205 [==============================] - 159s 776ms/step - loss: 1.2119 - acc: 0.6277 - val_loss: 0.8094 - val_acc: 0.6921\n",
            "Epoch 11/20\n",
            "205/205 [==============================] - 160s 780ms/step - loss: 1.1163 - acc: 0.6488 - val_loss: 0.7760 - val_acc: 0.7193\n",
            "Epoch 12/20\n",
            "205/205 [==============================] - 160s 780ms/step - loss: 1.1725 - acc: 0.6537 - val_loss: 0.8789 - val_acc: 0.6621\n",
            "Epoch 13/20\n",
            "205/205 [==============================] - 161s 783ms/step - loss: 1.1019 - acc: 0.6591 - val_loss: 0.7918 - val_acc: 0.7084\n",
            "Epoch 14/20\n",
            "205/205 [==============================] - 160s 781ms/step - loss: 1.0311 - acc: 0.6854 - val_loss: 0.7271 - val_acc: 0.7302\n",
            "Epoch 15/20\n",
            "205/205 [==============================] - 158s 769ms/step - loss: 1.0665 - acc: 0.6857 - val_loss: 0.6930 - val_acc: 0.7330\n",
            "Epoch 16/20\n",
            "205/205 [==============================] - 158s 768ms/step - loss: 1.0795 - acc: 0.6793 - val_loss: 0.7143 - val_acc: 0.7275\n",
            "Epoch 17/20\n",
            "205/205 [==============================] - 158s 772ms/step - loss: 1.0286 - acc: 0.7040 - val_loss: 0.7450 - val_acc: 0.7466\n",
            "Epoch 18/20\n",
            "205/205 [==============================] - 159s 777ms/step - loss: 0.9648 - acc: 0.7171 - val_loss: 0.7406 - val_acc: 0.7275\n",
            "Epoch 19/20\n",
            "205/205 [==============================] - 160s 780ms/step - loss: 0.9075 - acc: 0.7119 - val_loss: 0.6853 - val_acc: 0.7384\n",
            "Epoch 20/20\n",
            "205/205 [==============================] - 159s 775ms/step - loss: 0.8933 - acc: 0.7290 - val_loss: 0.7130 - val_acc: 0.7221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQJk_BgVzajr",
        "colab_type": "code",
        "outputId": "1c70cc67-26f6-46e4-f4d6-60894609cb28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                               validation_data=(valid_data, valid_labels),callbacks=[es, chkpt],\n",
        "                              class_weight={0:0.4040466, 1:1.9439528, 2:0.74128234, 3:3.89940828, 4:2.46816479})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "205/205 [==============================] - 162s 788ms/step - loss: 0.6407 - acc: 0.7933 - val_loss: 0.7926 - val_acc: 0.7439\n",
            "Epoch 2/20\n",
            "205/205 [==============================] - 158s 773ms/step - loss: 0.6521 - acc: 0.7997 - val_loss: 0.7294 - val_acc: 0.7575\n",
            "Epoch 3/20\n",
            "205/205 [==============================] - 159s 778ms/step - loss: 0.5892 - acc: 0.8137 - val_loss: 0.7587 - val_acc: 0.7657\n",
            "Epoch 4/20\n",
            "205/205 [==============================] - 160s 780ms/step - loss: 0.5786 - acc: 0.8128 - val_loss: 0.7746 - val_acc: 0.7738\n",
            "Epoch 5/20\n",
            "205/205 [==============================] - 163s 793ms/step - loss: 0.5609 - acc: 0.8402 - val_loss: 0.7822 - val_acc: 0.7330\n",
            "Epoch 6/20\n",
            "205/205 [==============================] - 177s 862ms/step - loss: 0.5557 - acc: 0.8314 - val_loss: 0.7838 - val_acc: 0.7302\n",
            "Epoch 7/20\n",
            "205/205 [==============================] - 159s 776ms/step - loss: 0.5420 - acc: 0.8384 - val_loss: 0.7800 - val_acc: 0.7548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UshsC6iW-Bbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('kaggle-data/model/model_acc84.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pzHUkOR-jFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}