{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Retinopathy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MittalNeha/retinopathy-kaggle/blob/master/Retinopathy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5hbhyjaow9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34sbLmuW9QM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_kaggle_data():\n",
        "  os.environ['KAGGLE_USERNAME'] = \"mittalneha\"\n",
        "  os.environ['KAGGLE_KEY'] = \"59ec3f992f5fb4b510bebd8dea889381\"\n",
        "  \n",
        "  !kaggle competitions download -c aptos2019-blindness-detection\n",
        "  !kaggle datasets download -d keras/vgg16\n",
        "  \n",
        "  !mkdir kaggle-data\n",
        "  !mv sample_submission.csv kaggle-data/\n",
        "  !mv test.csv kaggle-data/\n",
        "  !mv train.csv kaggle-data/\n",
        "  !unzip test_images.zip -d kaggle-data/test_images\n",
        "  !unzip train_images.zip -d kaggle-data/train_images\n",
        "  \n",
        "  !unzip vgg16.zip -d kaggle-data/vgg16/\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZWWQIcaCUiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = \"kaggle-data\"\n",
        "# get_kaggle_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDF4tVHX-hii",
        "colab_type": "code",
        "outputId": "dc125c9c-164f-45b3-be7c-76884ce396d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import os\n",
        "import glob\n",
        "import h5py\n",
        "import shutil\n",
        "import imgaug as aug\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mimg\n",
        "import imgaug.augmenters as iaa\n",
        "from os import listdir, makedirs, getcwd, remove\n",
        "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, SeparableConv2D\n",
        "from keras.layers import GlobalMaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
        "import cv2\n",
        "from keras import backend as K\n",
        "color = sns.color_palette()\n",
        "%matplotlib inline\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fKC-e_PDaKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(os.listdir(data_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYGTQNJGBHBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Set the seed for hash based operations in python\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "# Set the numpy seed\n",
        "np.random.seed(111)\n",
        "\n",
        "# Disable multi-threading in tensorflow ops\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "\n",
        "# Set the random seed in tensorflow at graph level\n",
        "tf.set_random_seed(111)\n",
        "\n",
        "# Define a tensorflow session with above session configs\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "\n",
        "# Set the session in keras\n",
        "K.set_session(sess)\n",
        "\n",
        "# Make the augmentation sequence deterministic\n",
        "aug.seed(111)\n",
        "\n",
        "seed = 111"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b62u52mWBPbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data_dir = Path(data_dir)\n",
        "\n",
        "# Path to train directory (Fancy pathlib...no more os.path!!)\n",
        "train_dir = data_dir / 'train_images'\n",
        "\n",
        "# Path to validation directory\n",
        "#val_dir = data_dir / 'val'\n",
        "\n",
        "# Path to test directory\n",
        "test_dir = data_dir / 'test_images'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjiqiXqCFilK",
        "colab_type": "code",
        "outputId": "11145df0-d9da-427e-de80-8beee4432d43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data = pd.read_csv(data_dir/'train.csv')\n",
        "train_data.head()\n",
        "train_data.columns = ['id_code', 'label']\n",
        "train_data.columns"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id_code', 'label'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "360Ww8Hugnjp",
        "colab_type": "code",
        "outputId": "8a974aaa-0ed9-4bcd-eeca-0c5c58fa6d89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3662, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb3hSZYyxZ8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_image(img):\n",
        "  \n",
        "  if img.shape[2] ==1:\n",
        "    img = np.dstack([img, img, img])\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "  img1 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  ret, img2 = cv2.threshold(img1, 10, 255, cv2.THRESH_BINARY)\n",
        "  points = np.argwhere(img2!=0)\n",
        "  points = np.fliplr(points)\n",
        "  x, y, w, h = cv2.boundingRect(points)\n",
        "\n",
        "#   img_cropped = img1[y:y+h, x:x+w]\n",
        "  color_cropped = img[y:y+h, x:x+w]\n",
        "\n",
        "  clahe = cv2.createCLAHE(clipLimit=4.0)\n",
        "#   img3 = clahe.apply(img_cropped)\n",
        "  lab = cv2.cvtColor(color_cropped, cv2.COLOR_BGR2LAB)\n",
        "  lab_planes = cv2.split(lab)\n",
        "  lab_planes[0] = clahe.apply(lab_planes[0])\n",
        "\n",
        "#   plt.imshow(lab_planes[0], 'gray')\n",
        "\n",
        "  out = cv2.merge(lab_planes)\n",
        "  img3 = cv2.cvtColor(out, cv2.COLOR_LAB2BGR)\n",
        "  \n",
        "  img = cv2.resize(img3, (224,224), cv2.INTER_AREA)\n",
        "  \n",
        "  img = img.astype(np.float32)/255\n",
        "#   img = np.expand_dims(img, axis=2)\n",
        "  \n",
        "  return img\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55l0FIHyFrLi",
        "colab_type": "code",
        "outputId": "e75fc143-17db-43bb-c1a3-46ab07e5ecec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, val = train_test_split(train_data, test_size=0.1)\n",
        "\n",
        "valid_data = []\n",
        "valid_labels = []\n",
        "\n",
        "for idx, row in val.iterrows():\n",
        "    path = str(train_dir) + '/' + row['id_code']+'.png'\n",
        "    img = cv2.imread(path)\n",
        "    img = preprocess_image(img)\n",
        "#     img = img.astype(np.float32)/255.\n",
        "    label = to_categorical(int(row['label']), num_classes=5)\n",
        "    valid_data.append(img)\n",
        "    valid_labels.append(label)\n",
        "    \n",
        "valid_data = np.array(valid_data)\n",
        "valid_labels = np.array(valid_labels)\n",
        "\n",
        "print(\"Total number of validation examples: \", valid_data.shape)\n",
        "print(\"Total number of labels:\", valid_labels.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of validation examples:  (367, 224, 224, 3)\n",
            "Total number of labels: (367, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxVQh245Fs0b",
        "colab_type": "code",
        "outputId": "c529df57-97d8-471c-aecb-4b02535cd130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data = train\n",
        "train_data.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3295, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSiYE5pjF-Gi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Augmentation sequence \n",
        "seq = iaa.OneOf([\n",
        "    iaa.Fliplr(), # horizontal flips\n",
        "    iaa.Affine(rotate=20), # roatation\n",
        "    iaa.Multiply((1.2, 1.5))]) #random brightness"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-DGGzcKGB0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_gen(data, batch_size, mode='train'):\n",
        "    # Get total number of samples in the data\n",
        "    n = len(data)\n",
        "    steps = n//batch_size\n",
        "    \n",
        "    # Define two numpy arrays for containing batch data and labels\n",
        "    batch_data = np.zeros((batch_size, 224, 224, 3), dtype=np.float32)\n",
        "    batch_labels = np.zeros((batch_size,5), dtype=np.float32)\n",
        "\n",
        "    # Get a numpy array of all the indices of the input data\n",
        "    indices = np.arange(n)\n",
        "    \n",
        "    # Initialize a counter\n",
        "    i =0\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "        # Get the next batch \n",
        "        count = 0\n",
        "        next_batch = indices[(i*batch_size):(i+1)*batch_size]\n",
        "        for j, idx in enumerate(next_batch):\n",
        "            img_name = str(train_dir) + '/' + data.iloc[idx]['id_code']+'.png'\n",
        "            \n",
        "            label = data.iloc[idx]['label']\n",
        "            \n",
        "            # one hot encoding\n",
        "            encoded_label = to_categorical(label, num_classes=5)\n",
        "            # read the image and resize\n",
        "            img = cv2.imread(str(img_name))\n",
        "#             img = cv2.resize(img, (224,224))\n",
        "            \n",
        "            orig_img = preprocess_image(img)\n",
        "            # normalize the image pixels\n",
        "            orig_img = orig_img.astype(np.float32)/255.\n",
        "            \n",
        "            batch_data[count] = orig_img\n",
        "            batch_labels[count] = encoded_label\n",
        "            \n",
        "            # generating more samples of the undersampled class\n",
        "#             if label==0 and count < batch_size-2:\n",
        "#             if label!=0 and label!=2 and count < batch_size-3:\n",
        "            if label!=0 and count < batch_size-3 and mode=='train':\n",
        "                aug_img1 = seq.augment_image(img)\n",
        "                aug_img2 = seq.augment_image(img)\n",
        "        \n",
        "                aug_img1 = preprocess_image(aug_img1)\n",
        "                aug_img2 = preprocess_image(aug_img2)\n",
        "            \n",
        "#                 aug_img1 = aug_img1.astype(np.float32)/255.\n",
        "#                 aug_img2 = aug_img2.astype(np.float32)/255.\n",
        "\n",
        "                batch_data[count+1] = aug_img1\n",
        "                batch_labels[count+1] = encoded_label\n",
        "                batch_data[count+2] = aug_img2\n",
        "                batch_labels[count+2] = encoded_label\n",
        "                count +=3            \n",
        "            else:\n",
        "                count+=1\n",
        "\n",
        "            if count>=batch_size-1:\n",
        "                break\n",
        "            \n",
        "        i+=1\n",
        "        \n",
        "        yield batch_data, batch_labels\n",
        "            \n",
        "        if i>=steps:\n",
        "#             print(count)\n",
        "#             for kk in range(0,count):\n",
        "#                 plt.figure()\n",
        "#                 plt.imshow(batch_data[kk])\n",
        "#                 plt.title(str(kk))\n",
        "#             return\n",
        "            i=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XslYRaLeGGxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    input_img = Input(shape=(224,224,3), name='ImageInput')\n",
        "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_1')(input_img)\n",
        "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_2')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool1')(x)\n",
        "    \n",
        "    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_1')(x)\n",
        "    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_2')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool2')(x)\n",
        "    \n",
        "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_1')(x)\n",
        "    x = BatchNormalization(name='bn1')(x)\n",
        "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_2')(x)\n",
        "    x = BatchNormalization(name='bn2')(x)\n",
        "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_3')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool3')(x)\n",
        "    \n",
        "    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_1')(x)\n",
        "    x = BatchNormalization(name='bn3')(x)\n",
        "    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_2')(x)\n",
        "    x = BatchNormalization(name='bn4')(x)\n",
        "    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_3')(x)\n",
        "    x = MaxPooling2D((2,2), name='pool4')(x)\n",
        "    \n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(1024, activation='relu', name='fc1')(x)\n",
        "    x = Dropout(0.7, name='dropout1')(x)\n",
        "    x = Dense(512, activation='relu', name='fc2')(x)\n",
        "    x = Dropout(0.5, name='dropout2')(x)\n",
        "    x = Dense(5, activation='softmax', name='fc3')(x)\n",
        "    \n",
        "    model = Model(inputs=input_img, outputs=x)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH4HU114GIHq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "200ed2b7-f9be-41fe-8d63-f5a8996fbe31"
      },
      "source": [
        "model =  build_model()\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0724 11:18:06.093325 140577958295424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0724 11:18:06.103524 140577958295424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0724 11:18:06.145281 140577958295424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0724 11:18:06.256685 140577958295424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0724 11:18:06.258061 140577958295424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0724 11:18:06.264847 140577958295424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "W0724 11:18:08.182056 140577958295424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0724 11:18:08.855020 140577958295424 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0724 11:18:08.861488 140577958295424 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "ImageInput (InputLayer)      (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "Conv1_1 (Conv2D)             (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "Conv1_2 (Conv2D)             (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "Conv2_1 (SeparableConv2D)    (None, 112, 112, 128)     8896      \n",
            "_________________________________________________________________\n",
            "Conv2_2 (SeparableConv2D)    (None, 112, 112, 128)     17664     \n",
            "_________________________________________________________________\n",
            "pool2 (MaxPooling2D)         (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "Conv3_1 (SeparableConv2D)    (None, 56, 56, 256)       34176     \n",
            "_________________________________________________________________\n",
            "bn1 (BatchNormalization)     (None, 56, 56, 256)       1024      \n",
            "_________________________________________________________________\n",
            "Conv3_2 (SeparableConv2D)    (None, 56, 56, 256)       68096     \n",
            "_________________________________________________________________\n",
            "bn2 (BatchNormalization)     (None, 56, 56, 256)       1024      \n",
            "_________________________________________________________________\n",
            "Conv3_3 (SeparableConv2D)    (None, 56, 56, 256)       68096     \n",
            "_________________________________________________________________\n",
            "pool3 (MaxPooling2D)         (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "Conv4_1 (SeparableConv2D)    (None, 28, 28, 512)       133888    \n",
            "_________________________________________________________________\n",
            "bn3 (BatchNormalization)     (None, 28, 28, 512)       2048      \n",
            "_________________________________________________________________\n",
            "Conv4_2 (SeparableConv2D)    (None, 28, 28, 512)       267264    \n",
            "_________________________________________________________________\n",
            "bn4 (BatchNormalization)     (None, 28, 28, 512)       2048      \n",
            "_________________________________________________________________\n",
            "Conv4_3 (SeparableConv2D)    (None, 28, 28, 512)       267264    \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 100352)            0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 1024)              102761472 \n",
            "_________________________________________________________________\n",
            "dropout1 (Dropout)           (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout2 (Dropout)           (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "fc3 (Dense)                  (None, 5)                 2565      \n",
            "=================================================================\n",
            "Total params: 104,199,045\n",
            "Trainable params: 104,195,973\n",
            "Non-trainable params: 3,072\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0o1ZKw3GKaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Open the VGG16 weight file\n",
        "f = h5py.File('kaggle-data/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', 'r')\n",
        "\n",
        "# Select the layers for which you want to set weight.\n",
        "\n",
        "w,b = f['block1_conv1']['block1_conv1_W_1:0'], f['block1_conv1']['block1_conv1_b_1:0']\n",
        "model.layers[1].set_weights = [w,b]\n",
        "\n",
        "w,b = f['block1_conv2']['block1_conv2_W_1:0'], f['block1_conv2']['block1_conv2_b_1:0']\n",
        "model.layers[2].set_weights = [w,b]\n",
        "\n",
        "w,b = f['block2_conv1']['block2_conv1_W_1:0'], f['block2_conv1']['block2_conv1_b_1:0']\n",
        "model.layers[4].set_weights = [w,b]\n",
        "\n",
        "w,b = f['block2_conv2']['block2_conv2_W_1:0'], f['block2_conv2']['block2_conv2_b_1:0']\n",
        "model.layers[5].set_weights = [w,b]\n",
        "\n",
        "f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK_cKafJ53np",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.save_weights('init_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyvAf3pAQpqi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1016234e-9d2d-4566-ae5f-c6b8659f2381"
      },
      "source": [
        "!mkdir kaggle-data/model/"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘kaggle-data/model/’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsKQsKgIetT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = Adam(lr=0.001, decay=1e-5)\n",
        "es = EarlyStopping(patience=5)\n",
        "# filepath=\"kaggle-data/model/weights-improvement-{epoch:02d}.hdf5\"\n",
        "filepath=\"kaggle-data/model/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "chkpt = ModelCheckpoint(filepath=filepath, save_best_only=True, save_weights_only=True)\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpFZmBTwcHz9",
        "colab_type": "text"
      },
      "source": [
        "**k-fold crossvalidation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILfLXwcC9zZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCAfnSsm937P",
        "colab_type": "code",
        "outputId": "9d3999f6-6a13-447a-ccd4-20aae245825f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# del model\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfCxfQB7oHOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from matplotlib import pyplot\n",
        "# pyplot.plot(history.history['loss'], label='train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwXPuejIcs4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(history.history['loss'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QECy6f1bITmV",
        "colab_type": "code",
        "outputId": "b5459d6e-8253-44e5-de50-9dcffc5dd9c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch_size = 16\n",
        "nb_epochs = 20\n",
        "\n",
        "# Get a train data generator\n",
        "train_data_gen = data_gen(data=train_data, batch_size=batch_size)\n",
        "\n",
        "# Define the number of training steps\n",
        "nb_train_steps = train_data.shape[0]//batch_size\n",
        "\n",
        "print(\"Number of training and validation steps: {} and {}\".format(nb_train_steps, len(valid_data)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training and validation steps: 205 and 367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSQ09bWV0n4s",
        "colab_type": "code",
        "outputId": "90fa8ce6-9d6a-482b-81a8-655cac956515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "valid_labels.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(367, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJYrUvt6eYMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_model(hist):\n",
        "  \n",
        "  #Plot the curves\n",
        "  N = len(hist.history['loss'])\n",
        "  plt.style.use(\"ggplot\")\n",
        "  plt.figure()\n",
        "  plt.plot(np.arange(0, N), hist.history[\"loss\"], label=\"train_loss\")\n",
        "  plt.plot(np.arange(0, N), hist.history[\"val_loss\"], label=\"val_loss\")\n",
        "  plt.plot(np.arange(0, N), hist.history[\"acc\"], label=\"train_acc\")\n",
        "  plt.plot(np.arange(0, N), hist.history[\"val_acc\"], label=\"val_acc\")\n",
        "  plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "  plt.xlabel(\"Epoch #\")\n",
        "  plt.ylabel(\"Loss/Accuracy\")\n",
        "  plt.legend(loc=\"lower left\")\n",
        "  \n",
        "  \n",
        "  #confusion matrix\n",
        "  preds = model.predict(eval_img, batch_size=16)\n",
        "  preds = np.argmax(preds, axis=-1)\n",
        "  \n",
        "  cm  = confusion_matrix(orig_eval_labels, preds)\n",
        "  plt.figure()\n",
        "  plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True, cmap=plt.cm.Blues, show_normed=True, show_absolute=False)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBaR_OzwkGiZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f12e6687-8dd4-489f-873e-1420a680951d"
      },
      "source": [
        "train_data.shape\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3295, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM4i6zZ9kI8X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "793444de-dc51-4418-d8bb-b1e19cf5514f"
      },
      "source": [
        "eval_data.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e5fdd8d6a77f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'eval_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ICyZ6T4kM9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am6C4QiQWAik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_data, val_data = train_test_split(train_data, test_size=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwTP7z7VWOX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluate with train data itself\n",
        "eval_img = []\n",
        "eval_labels = []\n",
        "\n",
        "for idx, row in eval_data.iterrows():\n",
        "    path = str(train_dir) + '/' + row['id_code']+'.png'\n",
        "    img = cv2.imread(path)\n",
        "    img = preprocess_image(img)\n",
        "    label = to_categorical(int(row['label']), num_classes=5)\n",
        "    eval_img.append(img)\n",
        "    eval_labels.append(label)\n",
        "    \n",
        "eval_img = np.array(eval_img)\n",
        "eval_labels = np.array(eval_labels)\n",
        "\n",
        "# Original labels\n",
        "orig_eval_labels = np.argmax(eval_labels, axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2Q7UR5ikpdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data.hist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GURph7mIGhlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.utils import class_weight\n",
        "# class_weight = class_weight.compute_class_weight('balanced', np.unique(orig_train_labels), orig_train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUG2nSKT8Udy",
        "colab_type": "code",
        "outputId": "602b442c-de47-4f69-e646-3eb76147ab58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# kappa_metrics = Metrics()\n",
        "history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                               validation_data=(valid_data, valid_labels),callbacks=[es, chkpt])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0724 11:19:21.235888 140577958295424 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "205/205 [==============================] - 911s 4s/step - loss: 1.3056 - acc: 0.4759 - val_loss: 2.0062 - val_acc: 0.2997\n",
            "Epoch 2/20\n",
            "205/205 [==============================] - 753s 4s/step - loss: 1.2334 - acc: 0.5271 - val_loss: 5.4330 - val_acc: 0.2997\n",
            "Epoch 3/20\n",
            "154/205 [=====================>........] - ETA: 3:03 - loss: 1.2484 - acc: 0.5110"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blEa0u5kbpW9",
        "colab_type": "code",
        "outputId": "3ca88237-984b-45fd-8e5a-3f96d9e3f89d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(history.history['loss'])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3gIa3p7GNfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit the model\n",
        "history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                               validation_data=(valid_data, valid_labels),callbacks=[es, chkpt], initial_epoch=len(history.history['loss']),\n",
        "                              class_weight={0:0.4040466, 1:1.9439528, 2:0.74128234, 3:3.89940828, 4:2.46816479})\n",
        "\n",
        "check_model(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC2zBuEnILuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                               validation_data=(valid_data, valid_labels),callbacks=[es, chkpt], initial_epoch=len(history.history['loss'],\n",
        "                              class_weight={0:0.4040466, 1:1.9439528, 2:0.74128234, 3:3.89940828, 4:2.46816479})\n",
        "\n",
        "check_model(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9GFZcY1GwCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check_model(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQJk_BgVzajr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                               validation_data=(valid_data, valid_labels),callbacks=[es, chkpt], initial_epoch=len(history.history['loss'],\n",
        "                              class_weight={0:0.4040466, 1:1.9439528, 2:0.74128234, 3:3.89940828, 4:2.46816479})\n",
        "\n",
        "check_model(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cJYR_bsm-hx",
        "colab_type": "text"
      },
      "source": [
        "**k-fold**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB714TT925YX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold_history = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoLBXbGp-mxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# K.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHDYd4XlcHbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "batch_size = 16\n",
        "nb_epochs = 20\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "# kappa_metrics = Metrics()\n",
        "i = 0\n",
        "init_epoch = 0\n",
        "\n",
        "for train_idx, val_idx in kfold.split(train_data, Y):\n",
        "  print(train_idx)\n",
        "  train_data_gen = data_gen(data=train_data.iloc[train_idx], batch_size=batch_size)\n",
        "  val_data_gen = data_gen(data=train_data.iloc[val_idx], batch_size=batch_size, mode='validation')\n",
        "  \n",
        "  nb_train_steps = len(train_data.iloc[train_idx]) //batch_size\n",
        "  nb_val_steps = len(train_data.iloc[val_idx]) //batch_size\n",
        "  \n",
        "#   model =  build_model()\n",
        "  model.load_weights('init_weights.h5')\n",
        "  model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=opt)\n",
        "  \n",
        "  r = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                               validation_data=val_data_gen,\n",
        "                                validation_steps=nb_val_steps, callbacks=[es, chkpt], \n",
        "                                class_weight={0:0.4040466, 1:1.9439528, 2:0.74128234, 3:3.89940828, 4:2.46816479})\n",
        "  kfold_history.append(r)\n",
        "#   del model\n",
        "  gc.collect()\n",
        "#   init_epoch += len(history.history['loss'])\n",
        "#   print(\"init_epoch = {}\".format(init_epoch))\n",
        "  i+=1\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvFwi4_w8w5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold_history.update(history.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWHAxWRM82vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSfTbRPfoVTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history.history['acc']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJsUqS3LvR2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def histogram(ratings, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Returns the counts of each type of rating that a rater made\n",
        "    \"\"\"\n",
        "    if min_rating is None:\n",
        "        min_rating = min(ratings)\n",
        "    if max_rating is None:\n",
        "        max_rating = max(ratings)\n",
        "    num_ratings = int(max_rating - min_rating + 1)\n",
        "    hist_ratings = [0 for x in range(num_ratings)]\n",
        "    for r in ratings:\n",
        "        hist_ratings[r - min_rating] += 1\n",
        "    return hist_ratings\n",
        "  \n",
        "def quadratic_weighted_kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Calculates the quadratic weighted kappa\n",
        "    quadratic_weighted_kappa calculates the quadratic weighted kappa\n",
        "    value, which is a measure of inter-rater agreement between two raters\n",
        "    that provide discrete numeric ratings.  Potential values range from -1\n",
        "    (representing complete disagreement) to 1 (representing complete\n",
        "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
        "    chance.\n",
        "\n",
        "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
        "    each correspond to a list of integer ratings.  These lists must have the\n",
        "    same length.\n",
        "\n",
        "    The ratings should be integers, and it is assumed that they contain\n",
        "    the complete range of possible ratings.\n",
        "\n",
        "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
        "    is the minimum possible rating, and max_rating is the maximum possible\n",
        "    rating\n",
        "    \"\"\"\n",
        "    rater_a = np.array(rater_a, dtype=int)\n",
        "    rater_b = np.array(rater_b, dtype=int)\n",
        "    assert(len(rater_a) == len(rater_b))\n",
        "    if min_rating is None:\n",
        "        min_rating = min(min(rater_a), min(rater_b))\n",
        "    if max_rating is None:\n",
        "        max_rating = max(max(rater_a), max(rater_b))\n",
        "    print(min_rating.shape)\n",
        "    print(max_rating.shape)\n",
        "    conf_mat = confusion_matrix(rater_a, rater_b)\n",
        "    num_ratings = len(conf_mat)\n",
        "    num_scored_items = float(len(rater_a))\n",
        "\n",
        "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
        "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
        "\n",
        "    numerator = 0.0\n",
        "    denominator = 0.0\n",
        "\n",
        "    for i in range(num_ratings):\n",
        "        for j in range(num_ratings):\n",
        "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
        "                              / num_scored_items)\n",
        "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
        "            numerator += d * conf_mat[i][j] / num_scored_items\n",
        "            denominator += d * expected_count / num_scored_items\n",
        "\n",
        "    return 1.0 - numerator / denominator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umBWXEqpvVAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Metrics(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.val_kappas = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        X_val, y_val = self.validation_data[:2]\n",
        "        y_val = np.argmax(y_val, axis=-1)\n",
        "#         y_val = y_val.astype(int).sum(axis=1) - 1\n",
        "        print(y_val.shape)\n",
        "        \n",
        "        y_pred = self.model.predict(X_val)\n",
        "        y_pred = np.argmax(y_pred, axis=-1)\n",
        "        print(y_pred.shape)\n",
        "#         print(y_pred[0])\n",
        "#         y_pred = y_pred.astype(int).sum(axis=1) - 1\n",
        "        \n",
        "#         print(y_pred.shape)\n",
        "        _val_kappa = quadratic_weighted_kappa(y_val, y_pred)\n",
        "\n",
        "#         _val_kappa = cohen_kappa_score(\n",
        "#             y_val,\n",
        "#             y_pred, \n",
        "#             weights='quadratic'\n",
        "#         )\n",
        "\n",
        "        self.val_kappas.append(_val_kappa)\n",
        "\n",
        "        print(f\"val_kappa: {_val_kappa:.4f}\")\n",
        "        \n",
        "        if _val_kappa == max(self.val_kappas):\n",
        "            print(\"Validation Kappa has improved. Saving model.\")\n",
        "            self.model.save('kaggle-data/model/model.h5')\n",
        "\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}